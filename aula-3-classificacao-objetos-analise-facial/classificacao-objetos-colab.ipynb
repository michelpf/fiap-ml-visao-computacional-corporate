{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação de objetos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Atenção: este notebook foi desenhado para funcionar no **Google Collab**. Se pretende executar localmente prefira a versão local deste notebook, sem o sufixo ```-collab```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Requerimentos\n",
    "\n",
    "Todas as bibliotecas já estão instaladas no Google Colab.\n",
    "\n",
    "* OpenCV >= 3.4.3\n",
    "* Matplotlib >= 3.1.3\n",
    "* Seaborn >= 0.0.10\n",
    "* Numpy >= 1.18.1\n",
    "\n",
    "### 1.2 Arquivos\n",
    "\n",
    "Baixe o repositório do GitHub utilizando o comando abaixo. Em caso de atualização, utilize o comando para apagar o diretório antes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf fiap-ml-visao-computacional/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/michelpf/fiap-ml-visao-computacional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos agora posicionar o diretório do repositório para a aula respectiva. Nesse caso envie o comando de mudança de diretório."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd fiap-ml-visao-computacional/aula-4-classificacao-objetos-analise-facial/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação das bibliotecas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cv2\n",
    "\n",
    "#Exibição na mesma tela do Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import dlib\n",
    "from scipy.spatial import distance as dist\n",
    "\n",
    "plt.style.use('seaborn')\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregando um classificador pré-treinado de Haar.\n",
    "\n",
    "Analise posteriormente outros classificadores disponíveis, dentre eles, classificador de pessoas, automóveis, gatos, sorriso, olhos, etc. neste repositório oficial do OpenCV https://github.com/opencv/opencv/tree/master/data/haarcascades."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Classificador de Viola-Jones\n",
    "\n",
    "Este classificador é especializado em identificar faces, também é conhecido como classificador Viola James e pode ser utilizado em outras aplicações. O paper original pode ser baixado [aqui](https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf). \n",
    "\n",
    "Devido a característica deste tipo de classificador, sua identificação é extramamente rápida, com identificação < 0,02s, aplicações em sistemas em tempo real, especialmente câmeras de vigilância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Classificador de Faces\n",
    "\n",
    "A aplicação mais utilizada do classificador em cascata de Viola-Jones (ou classificador de Haar) é para encontrar faces em imagens ou vídeos, especialmente por sua identificação ser bem rápida.\n",
    "\n",
    "Este tipo de classificador serve para separar a região de interesse de uma determinada área, para posteriormente, aplicar outros classificadores que poderão, por exemplo, classificar de quem é o rosto, uma vez que a região foi separado da imagem original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregando classifcador\n",
    "classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "imagem = cv2.imread('imagens/people.jpg')\n",
    "\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retornará a região de interesse da face identificada como tupla, armazenando as coordenadas superiores esquerda e inferior \n",
    "# direita.\n",
    "# Se retornar vazio é por que não há faces identificadas.\n",
    "# Os valores padrão são configurações inciais recomendadas \n",
    "# (cv.HaarDetectObjects(image, cascade, storage, scale_factor=1.1, min_neighbors=3, flags=0, min_size=(0, 0)))\n",
    "\n",
    "faces = classificador_face.detectMultiScale(imagem_gray, 1.3, 5)\n",
    "\n",
    "# Lista de faces. Caso não seja identificada será retornado None (nulo)\n",
    "if faces is None:\n",
    "     cv2.putText(imagem, \"Rosto ausente\", (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,0), 6)\n",
    "    \n",
    "# Desenhando retângulos nos rostos identificados\n",
    "for (x,y,w,h) in faces:\n",
    "    cv2.rectangle(imagem, (x,y), (x+w,y+h), (255,255,0), 2)\n",
    "    \n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.axis('off')\n",
    "plt.title(\"Faces identificadas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construindo funções para utilizar de forma modular em análise de imagens ou vídeos. \n",
    "\n",
    "Optamos por criar 2 versões da mesma função, sendo a com o sufixo **rgb** espera como parâmetro de entrada uma imagem neste formato. A outra função espera como imagem de entrada no formato **bgr**, que é o formato padrão para escrita de vídeo.\n",
    "\n",
    "Função de análise e inferência para imagens ou frames no formato RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_rosto_rgb(imagem):\n",
    "    classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.2, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(imagem, (x,y), (x+w,y+h), (255,255,0), 2)\n",
    "\n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de análise e inferência para imagens ou frames no formato BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_rosto_bgr(imagem):\n",
    "    classificador_face = cv2.CascadeClassifier('classificadores/haarcascade_frontalface_default.xml')\n",
    "\n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    faces = classificador_face.detectMultiScale(imagem_gray, 1.2, 5)\n",
    "\n",
    "    for (x,y,w,h) in faces:\n",
    "        cv2.rectangle(imagem, (x,y), (x+w,y+h), (0,255,255), 2)\n",
    "\n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicação do classificador em cascata para leitura e processamento de vídeo. A análise é realizada frame-a-frame e o resultado do processamento é salvo em outro vídeo para posteiror verificação.\n",
    "Como a escrita de arquivos é feita no formato BGR, optamos por usar a função com este sufixo (bgr) para o processamento. Poderíamos usar a funçÃo RGB, mas nesse caso precisaríamos de converter cada frame para RGB e depois para BGR, o que consumiria muito processamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"videos/people-talking.mp4\")\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"Frames per second (FPS): \" + str(video_fps))\n",
    "\n",
    "writer = None\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        frame = identificar_rosto_bgr(frame)\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(\"videos/people-talking-processed.avi\", fourcc, video_fps, (frame.shape[1], frame.shape[0]), True)\n",
    "        writer.write(frame)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a uma imagem coletada de uma câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Javascript\n",
    "from google.colab.output import eval_js\n",
    "from base64 import b64decode\n",
    "\n",
    "def take_photo(filename='photo.jpg', quality=0.8):\n",
    "  js = Javascript('''\n",
    "    async function takePhoto(quality) {\n",
    "      const div = document.createElement('div');\n",
    "      const capture = document.createElement('button');\n",
    "      capture.textContent = 'Capture';\n",
    "      div.appendChild(capture);\n",
    "\n",
    "      const video = document.createElement('video');\n",
    "      video.style.display = 'block';\n",
    "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
    "\n",
    "      document.body.appendChild(div);\n",
    "      div.appendChild(video);\n",
    "      video.srcObject = stream;\n",
    "      await video.play();\n",
    "\n",
    "      // Resize the output to fit the video element.\n",
    "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
    "\n",
    "      // Wait for Capture to be clicked.\n",
    "      await new Promise((resolve) => capture.onclick = resolve);\n",
    "\n",
    "      const canvas = document.createElement('canvas');\n",
    "      canvas.width = video.videoWidth;\n",
    "      canvas.height = video.videoHeight;\n",
    "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
    "      stream.getVideoTracks()[0].stop();\n",
    "      div.remove();\n",
    "      return canvas.toDataURL('image/jpeg', quality);\n",
    "    }\n",
    "    ''')\n",
    "  display(js)\n",
    "  data = eval_js('takePhoto({})'.format(quality))\n",
    "  binary = b64decode(data.split(',')[1])\n",
    "  with open(filename, 'wb') as f:\n",
    "    f.write(binary)\n",
    "  return filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "try:\n",
    "  filename = take_photo(\"imagens/foto.jpg\")\n",
    "  print('Saved to {}'.format(filename))\n",
    "  \n",
    "  # Show the image which was just taken.\n",
    "  display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/foto.jpg\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "imagem_rosto = identificar_rosto_rgb(imagem)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem_rosto, cmap=\"gray\")\n",
    "plt.title(\"Imagem com rosto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Classificador em Cascata Customizado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construir um classificador em cascata especializado consiste em reunir imagens de treino positivas e negativas, diferentemente de outros tipos de classificadores, como por exemplo os de _deep learning_. \n",
    "\n",
    "O processo de construção é realizado por meio de utilitários em linha de comando do OpenCV, que apesar de não ser complexo pode envolver uma dedicação maior. Por tal razão, vamos optar pelo [Cascade Trainer GUI](http://amin-ahmadi.com/cascade-trainer-gui/), de Amin Ahmadi, criado para ser executado em Windows que facilita muito este processo. http://amin-ahmadi.com/cascade-trainer-gui/.\n",
    "\n",
    "As imagens para as etapas de treino podem ser baixadas em http://www.dis.uniroma1.it/~labrococo/?q=node/459. \n",
    "\n",
    "Função de análise e inferência para imagens ou frames no formato RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_bola_rgb(imagem):\n",
    "    ball_classifier = cv2.CascadeClassifier('ball/classifier/cascade.xml')\n",
    "  \n",
    "    gray = cv2.cvtColor(imagem, cv2.COLOR_RGB2GRAY)\n",
    "    balls = ball_classifier.detectMultiScale(gray, 1.3, 3)\n",
    "\n",
    "    for (x,y,w,h) in balls:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = int(w) * int(h)\n",
    "        \n",
    "        if area > 1500:\n",
    "            cv2.rectangle(imagem, (x,y), (x+w,y+h), (255,255,0), 1)\n",
    "        \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de análise e inferência para imagens ou frames no formato BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_bola_bgr(imagem):\n",
    "    ball_classifier = cv2.CascadeClassifier('ball/classifier/cascade.xml')\n",
    "  \n",
    "    gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    balls = ball_classifier.detectMultiScale(gray, 1.3, 3)\n",
    "    \n",
    "    for (x,y,w,h) in balls:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = int(w) * int(h)\n",
    "        \n",
    "        if area > 1500:\n",
    "            cv2.rectangle(imagem, (x,y), (x+w,y+h), (0,255,255), 1)\n",
    "        \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/soccer-ball.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.axis('off')\n",
    "plt.title(\"Bola de futebol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_proc = identificar_bola_rgb(imagem)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(imagem_proc)\n",
    "plt.axis('off')\n",
    "plt.title(\"Bola de futebol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processamento de vídeo utilizando o classificador customizado. Neste vídeo temos os mesmos elementos utilizados na classificação, como a bola (objeto classificado, positivo) e o fundo (objeto não desejado, faz parte do conjunto de imagens negativas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"videos/soccer.avi\")\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"Frames per second (FPS): \" + str(video_fps))\n",
    "\n",
    "writer = None\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        frame = identificar_bola_bgr(frame)\n",
    "\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(\"videos/soccer-processed.avi\", fourcc, video_fps, (frame.shape[1], frame.shape[0]), True)\n",
    "        writer.write(frame)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Classificador de Pessoas\n",
    "\n",
    "Este classificador identifica pessoas (corpos inteiros). Sua utilidade é para segmentar e localizar uma ou mais pessoas em uma determinada cena.\n",
    "\n",
    "Repare que pessoas muito próximas uma das outras prejudica a identificação. Em vídeos de vigilância, este aspecto negativo é diminuído, pois ha muitas oportunidades do classificador inferir, e conseguir identificar as pessoas em outros momentos onde estão mais separadas umas das outroas.\n",
    "\n",
    "Função de análise e inferência para imagens ou frames no formato RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_pessoas_rgb(imagem):\n",
    "    classificador_pessoas = cv2.CascadeClassifier('classificadores/haarcascade_fullbody.xml')\n",
    "    \n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_RGB2GRAY)\n",
    "    pessoas = classificador_pessoas.detectMultiScale(imagem_gray, 1.2, 3)\n",
    "\n",
    "    for (x,y,w,h) in pessoas:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = int(w) * int(h)\n",
    "        cv2.rectangle(imagem, (x,y), (x+w,y+h), (255,255,0), 2)\n",
    "        \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de análise e inferência para imagens ou frames no formato BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_pessoas_bgr(imagem):\n",
    "    classificador_pessoas = cv2.CascadeClassifier('classificadores/haarcascade_fullbody.xml')\n",
    "    \n",
    "    imagem_gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    pessoas = classificador_pessoas.detectMultiScale(imagem_gray, 1.2, 3)\n",
    "\n",
    "    for (x,y,w,h) in pessoas:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = int(w) * int(h)\n",
    "        cv2.rectangle(imagem, (x,y), (x+w,y+h), (0,255,255), 2)\n",
    "        \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/people-walking.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem)\n",
    "plt.axis('off')\n",
    "plt.title(\"Pessoas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_proc = identificar_pessoas_rgb(imagem)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(imagem_proc)\n",
    "plt.axis('off')\n",
    "plt.title(\"Pessoas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando vídeo para classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"videos/walking.avi\")\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"Frames per second (FPS): \" + str(video_fps))\n",
    "\n",
    "writer = None\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        frame = identificar_pessoas_bgr(frame)\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(\"videos/people-walking-processed.avi\", fourcc, video_fps, (frame.shape[1], frame.shape[0]), True)\n",
    "        writer.write(frame)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Classificador em Cascata de Automóveis\n",
    "\n",
    "Classificador especializado em identificar automóveis.\n",
    "\n",
    "Função de análise e inferência para imagens ou frames no formato RGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_carros_rgb(imagem):\n",
    "    cars_classifier = cv2.CascadeClassifier('classificadores/haarcascade_car.xml')\n",
    "  \n",
    "    gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    cars = cars_classifier.detectMultiScale(gray, 1.2, 3)\n",
    "\n",
    "    for (x,y,w,h) in cars:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = int(w) * int(h)\n",
    "        cv2.rectangle(imagem, (x,y), (x+w,y+h), (255,255,0), 1)\n",
    "        \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função de análise e inferência para imagens ou frames no formato BGR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identificar_carros_bgr(imagem):\n",
    "    cars_classifier = cv2.CascadeClassifier('classificadores/haarcascade_car.xml')\n",
    "  \n",
    "    gray = cv2.cvtColor(imagem, cv2.COLOR_BGR2GRAY)\n",
    "    cars = cars_classifier.detectMultiScale(gray, 1.2, 3)\n",
    "\n",
    "    for (x,y,w,h) in cars:\n",
    "        roi = imagem[y:y+h, x:x+w]\n",
    "        area = int(w) * int(h)\n",
    "        cv2.rectangle(imagem, (x,y), (x+w,y+h), (0,255,255), 1)\n",
    "        \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/cars.png\")\n",
    "imagem = cv2.cvtColor(imagem, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem)\n",
    "plt.title(\"Carros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem_proc = identificar_carros_rgb(imagem)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.imshow(imagem_proc)\n",
    "plt.axis('off')\n",
    "plt.title(\"Carros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Análise de vídeo de câmera de uma estrada e inferência do classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video = cv2.VideoCapture(\"videos/cars.avi\")\n",
    "video_fps = video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "print(\"Frames per second (FPS): \" + str(video_fps))\n",
    "\n",
    "writer = None\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
    "\n",
    "while True:\n",
    "    ret, frame = video.read()\n",
    "    if ret:\n",
    "        frame = identificar_carros_bgr(frame)\n",
    "        if writer is None:\n",
    "            writer = cv2.VideoWriter(\"videos/cars-processed.avi\", fourcc, video_fps, (frame.shape[1], frame.shape[0]), True)\n",
    "        writer.write(frame)\n",
    "    else:\n",
    "        break\n",
    "        \n",
    "video.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identificação de Marcos Faciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O toolkit [Dlib](http://dlib.net) fornece uma série de bibliotecas e ferramentas para utilização em problemas de visão computacional. Para o uso de identificação de marcos faciais vamos utilizar sua biblioteca responsável por identificar faces de uma imagem e posterior identificação dos marcos faciais.\n",
    "\n",
    "O Dlib vem instalado por padrão na plataforma Google Colab, não é necesário baixar.\n",
    "\n",
    "Os modelos podem ser baixados neste endereço http://dlib.net/files/. Os que serão utilizados estão já incluídos na pasta ```modelos```.\n",
    "\n",
    "Iniciamos o processo de identificação configurando o modelo treinado de 68 pontos ```shape_predictor_68_face_landmarks.dat```. Após isso precisamos identificar a face e para cada face identificada retornar a lista de pontos, na variável ```marcos_faciais```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_68_path = \"modelos/shape_predictor_68_face_landmarks.dat\"\n",
    "classificador_5_path = \"modelos/shape_predictor_5_face_landmarks.dat\"\n",
    "\n",
    "classificador_dlib_68 = dlib.shape_predictor(classificador_68_path)\n",
    "classificador_dlib_5 = dlib.shape_predictor(classificador_5_path)\n",
    "\n",
    "detector_face_dlib = dlib.get_frontal_face_detector()\n",
    "\n",
    "def obter_marcos(imagem, marcos_68_pontos=True):\n",
    "    \n",
    "    classificador_dlib = classificador_dlib_68\n",
    "    \n",
    "    if marcos_68_pontos is False:\n",
    "        classificador_dlib = classificador_dlib_5\n",
    "        \n",
    "    faces = detector_face_dlib(imagem, 1)\n",
    "\n",
    "    if len(faces) == 0:\n",
    "        print(\"Não foi encontrada nenhuma face.\")\n",
    "        return None\n",
    "        \n",
    "    marcos_faciais = []\n",
    "    \n",
    "    for face in faces:\n",
    "        marcos_faciais.append(np.matrix([[p.x, p.y] for p in classificador_dlib(imagem, face).parts()]))\n",
    "\n",
    "    return marcos_faciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Com os pontos mapeados, o próximo passo é criar uma função para anotar em uma nova imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anotar_marcos(imagem, marcos_faciais):\n",
    "    \n",
    "    if marcos_faciais is None:\n",
    "        print(\"Não foi identificado nenhum marco facial.\")\n",
    "        return imagem\n",
    "    \n",
    "    for marco_facial in marcos_faciais:\n",
    "        for idx, ponto in enumerate(marco_facial):\n",
    "            centro = (ponto[0,0], ponto[0,1])\n",
    "            \n",
    "            cv2.putText(imagem, str(idx), centro, fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                        fontScale=0.5, color=(255, 255, 255))\n",
    "            cv2.circle(imagem, centro, 3, color=(0, 255, 255), thickness=-1)\n",
    "            \n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/girl.jpg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem)\n",
    "imagem_marcos = anotar_marcos(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais com 68 pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/girl.jpg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem, False)\n",
    "imagem_marcos = anotar_marcos(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais com 5 pontos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicando a detecção e anotação utilizando uma câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "try:\n",
    "  filename = take_photo(\"imagens/foto.jpg\")\n",
    "  print('Saved to {}'.format(filename))\n",
    "  \n",
    "  # Show the image which was just taken.\n",
    "  display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/foto.jpg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem)\n",
    "imagem_marcos = anotar_marcos(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais da câmera\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Análise de Marcos Faciais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os 68 pontos estão divididos nos seguintes coponentes do rosto humano:\n",
    "\n",
    "* Face completa ```1 ao 68```\n",
    "* Sombrancelha direita ```17 ao 21```\n",
    "* Sombracelha esquerda ```22 ao 26```\n",
    "* Olho direito ```36 ao 41```\n",
    "* Olho esquerdo ```42 ao 47```\n",
    "* Nariz ```27 ao 34```\n",
    "* Lábio ```48 ao 60```\n",
    "* Mandíbula ```1 ao 16```\n",
    "\n",
    "Os pontos que o DLib retorna são zero based, portanto o primeiro ponto começa no 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACE_COMPLETA = list(range(0, 68))\n",
    "SOMBRANCELHA_DIREITA = list(range(17, 22))\n",
    "SOMBRANCELHA_ESQUERDA = list(range(22, 27))\n",
    "OLHO_DIREITO = list(range(36, 42))\n",
    "OLHO_ESQUERDO = list(range(42, 48))\n",
    "NARIZ = list(range(27, 35))\n",
    "LABIO = list(range(48, 61))\n",
    "MANDIBULA = list(range(0, 17))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Função para criar polígonos a partir dos pontos de identificação. Estes polígonos podem oferecer ferramentas como cálculo de área para identificação melhor das estruturas faciais, como piscadas, movimentação da boca, dentre outros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anotar_marcos_faciais_componente(imagem, marcos_faciais):\n",
    "\n",
    "    if marcos_faciais is None:\n",
    "        print(\"Não foi identificado nenhum marco facial.\")\n",
    "        return imagem\n",
    "    \n",
    "    for marco_facial in marcos_faciais:\n",
    "        for k, d in enumerate(marco_facial):\n",
    "            pontos = cv2.convexHull(marco_facial[FACE_COMPLETA])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "            pontos = cv2.convexHull(marco_facial[NARIZ])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "            pontos = cv2.convexHull(marco_facial[LABIO])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "            pontos = cv2.convexHull(marco_facial[SOMBRANCELHA_DIREITA])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "            pontos = cv2.convexHull(marco_facial[SOMBRANCELHA_ESQUERDA])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "            pontos = cv2.convexHull(marco_facial[OLHO_ESQUERDO])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "            pontos = cv2.convexHull(marco_facial[OLHO_DIREITO])\n",
    "            cv2.drawContours(imagem, [pontos], 0, (0, 255, 0), 2)\n",
    "\n",
    "    return imagem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/cervero.jpeg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem)\n",
    "imagem_marcos = anotar_marcos_faciais_componente(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais com 68 pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/girl.jpg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem)\n",
    "imagem_marcos = anotar_marcos_faciais_componente(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais com 68 pontos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para lidar com situações onde o rosto capturado pode se aproximar ou se afastar da câmera utilizamos um cálculo de aspecto de razão. Esse cálculo divide duas medidas e sua razão é utilizada como medida para análises.\n",
    "\n",
    "Uma vez que temos os pontos em volta de cada componente do rosto humano é possível realizar essas medidas de aspecto de razão para analisar a abertura dos olhos, por exemplo.\n",
    "\n",
    "Nesse sentido, calculamos a medida de aspecto razão dos olhos ou EAR. Essa medida foi discutida neste [paper de Soukupová and Čech](http://vision.fe.uni-lj.si/cvww2016/proceedings/papers/05.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ear(pontos_olho):\n",
    "    \n",
    "    a = dist.euclidean(pontos_olho[1], pontos_olho[5])\n",
    "    b = dist.euclidean(pontos_olho[2], pontos_olho[4])\n",
    "    c = dist.euclidean(pontos_olho[0], pontos_olho[3])\n",
    "\n",
    "    medida_ear = (a + b) / (2.0 * c)\n",
    "\n",
    "    return medida_ear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos inspecionar a abertura dos olhos da imagem analisada. Note que como existe apenas uma única pessoa (ou rosto) na imagem,  utilizaremos o índice 0 somente. Se houvessem mais pessoas, poderíamos analisar cada uma delas iterando sobre o índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_olho_direito = ear(marcos_faciais[0][OLHO_DIREITO])\n",
    "ear_olho_esquerdo = ear(marcos_faciais[0][OLHO_ESQUERDO])\n",
    "\n",
    "print(\"EAR do olho esquerdo \" + str(ear_olho_esquerdo))\n",
    "print(\"EAR do olho direito \" + str(ear_olho_direito))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/mulher-olhos-fechados.jpg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem)\n",
    "imagem_marcos = anotar_marcos_faciais_componente(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais com 68 pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_olho_direito = ear(marcos_faciais[0][OLHO_DIREITO])\n",
    "ear_olho_esquerdo = ear(marcos_faciais[0][OLHO_ESQUERDO])\n",
    "\n",
    "print(\"EAR do olho esquerdo \" + str(ear_olho_esquerdo))\n",
    "print(\"EAR do olho direito \" + str(ear_olho_direito))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicar a análise de face e marcos faciais utilizando a câmera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "try:\n",
    "  filename = take_photo(\"imagens/foto.jpg\")\n",
    "  print('Saved to {}'.format(filename))\n",
    "  \n",
    "  # Show the image which was just taken.\n",
    "  display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagem = cv2.imread(\"imagens/foto.jpg\")\n",
    "\n",
    "marcos_faciais = obter_marcos(imagem)\n",
    "imagem_marcos = anotar_marcos_faciais_componente(imagem, marcos_faciais)\n",
    "\n",
    "imagem_marcos = cv2.cvtColor(imagem_marcos, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.figure(figsize=(30,20))\n",
    "plt.axis('off')\n",
    "plt.imshow(imagem_marcos)\n",
    "plt.title(\"Marcos faciais com 68 pontos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ear_olho_direito = ear(marcos_faciais[0][OLHO_DIREITO])\n",
    "ear_olho_esquerdo = ear(marcos_faciais[0][OLHO_ESQUERDO])\n",
    "\n",
    "print(\"EAR do olho esquerdo \" + str(ear_olho_esquerdo))\n",
    "print(\"EAR do olho direito \" + str(ear_olho_direito))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Troca de rostos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Troca de rostos. Inicalmente utilizamos os marcadores da face para recortar adequadamente os rotos e posteriormente implantamos em outra imagem, aplicando suavizações em suas bordas.\n",
    "\n",
    "_Retirado integralmente de [Matthew Earl](http://matthewearl.github.io/2015/07/28/switching-eds-with-python/)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "SCALE_FACTOR = 1 \n",
    "FEATHER_AMOUNT = 11\n",
    "\n",
    "FACE_POINTS = list(range(17, 68))\n",
    "MOUTH_POINTS = list(range(48, 61))\n",
    "RIGHT_BROW_POINTS = list(range(17, 22))\n",
    "LEFT_BROW_POINTS = list(range(22, 27))\n",
    "RIGHT_EYE_POINTS = list(range(36, 42))\n",
    "LEFT_EYE_POINTS = list(range(42, 48))\n",
    "NOSE_POINTS = list(range(27, 35))\n",
    "JAW_POINTS = list(range(0, 17))\n",
    "\n",
    "# Points used to line up the images.\n",
    "ALIGN_POINTS = (LEFT_BROW_POINTS + RIGHT_EYE_POINTS + LEFT_EYE_POINTS + RIGHT_BROW_POINTS + NOSE_POINTS + MOUTH_POINTS)\n",
    "\n",
    "# Points from the second image to overlay on the first. The convex hull of each\n",
    "# element will be overlaid.\n",
    "OVERLAY_POINTS = [\n",
    "    LEFT_EYE_POINTS + RIGHT_EYE_POINTS + LEFT_BROW_POINTS + RIGHT_BROW_POINTS,\n",
    "    NOSE_POINTS + MOUTH_POINTS,\n",
    "]\n",
    "\n",
    "# Amount of blur to use during colour correction, as a fraction of the\n",
    "# pupillary distance.\n",
    "COLOUR_CORRECT_BLUR_FRAC = 0.6\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(classificador_68_path)\n",
    "\n",
    "def get_landmarks(im):\n",
    "    # Returns facial landmarks as (x,y) coordinates\n",
    "    rects = detector(im, 1)\n",
    "    \n",
    "    if len(rects) > 1:\n",
    "        return im\n",
    "    if len(rects) == 0:\n",
    "        return im\n",
    "\n",
    "    return numpy.matrix([[p.x, p.y] for p in predictor(im, rects[0]).parts()])\n",
    "\n",
    "\n",
    "def annotate_landmarks(im, landmarks):\n",
    "    #Overlays the landmark points on the image itself\n",
    "    \n",
    "    im = im.copy()\n",
    "    for idx, point in enumerate(landmarks):\n",
    "        pos = (point[0, 0], point[0, 1])\n",
    "        cv2.putText(im, str(idx), pos,\n",
    "                    fontFace=cv2.FONT_HERSHEY_SCRIPT_SIMPLEX,\n",
    "                    fontScale=0.4,\n",
    "                    color=(0, 0, 255))\n",
    "        cv2.circle(im, pos, 3, color=(0, 255, 255))\n",
    "    return im\n",
    "\n",
    "def draw_convex_hull(im, points, color):\n",
    "    points = cv2.convexHull(points)\n",
    "    cv2.fillConvexPoly(im, points, color=color)\n",
    "\n",
    "def get_face_mask(im, landmarks):\n",
    "    im = numpy.zeros(im.shape[:2], dtype=numpy.float64)\n",
    "\n",
    "    for group in OVERLAY_POINTS:\n",
    "        draw_convex_hull(im,\n",
    "                         landmarks[group],\n",
    "                         color=1)\n",
    "\n",
    "    im = numpy.array([im, im, im]).transpose((1, 2, 0))\n",
    "\n",
    "    im = (cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0) > 0) * 1.0\n",
    "    im = cv2.GaussianBlur(im, (FEATHER_AMOUNT, FEATHER_AMOUNT), 0)\n",
    "\n",
    "    return im\n",
    "    \n",
    "def transformation_from_points(points1, points2):\n",
    "    # Solve the procrustes problem by subtracting centroids, scaling by the\n",
    "    # standard deviation, and then using the SVD to calculate the rotation. See\n",
    "    # the following for more details:\n",
    "    #   https://en.wikipedia.org/wiki/Orthogonal_Procrustes_problem\n",
    "\n",
    "    points1 = points1.astype(numpy.float64)\n",
    "    points2 = points2.astype(numpy.float64)\n",
    "\n",
    "    c1 = numpy.mean(points1, axis=0)\n",
    "    c2 = numpy.mean(points2, axis=0)\n",
    "    points1 -= c1\n",
    "    points2 -= c2\n",
    "\n",
    "    s1 = numpy.std(points1)\n",
    "    s2 = numpy.std(points2)\n",
    "    points1 /= s1\n",
    "    points2 /= s2\n",
    "\n",
    "    U, S, Vt = numpy.linalg.svd(points1.T * points2)\n",
    "\n",
    "    # The R we seek is in fact the transpose of the one given by U * Vt. This\n",
    "    # is because the above formulation assumes the matrix goes on the right\n",
    "    # (with row vectors) where as our solution requires the matrix to be on the\n",
    "    # left (with column vectors).\n",
    "    R = (U * Vt).T\n",
    "\n",
    "    return numpy.vstack([numpy.hstack(((s2 / s1) * R,\n",
    "                                       c2.T - (s2 / s1) * R * c1.T)),\n",
    "                         numpy.matrix([0., 0., 1.])])\n",
    "\n",
    "def read_im_and_landmarks(image):\n",
    "    im = image\n",
    "    im = cv2.resize(im,None,fx=1, fy=1, interpolation = cv2.INTER_LINEAR)\n",
    "    im = cv2.resize(im, (im.shape[1] * SCALE_FACTOR,\n",
    "                         im.shape[0] * SCALE_FACTOR))\n",
    "    s = get_landmarks(im)\n",
    "\n",
    "    return im, s\n",
    "\n",
    "def warp_im(im, M, dshape):\n",
    "    output_im = numpy.zeros(dshape, dtype=im.dtype)\n",
    "    cv2.warpAffine(im,\n",
    "                   M[:2],\n",
    "                   (dshape[1], dshape[0]),\n",
    "                   dst=output_im,\n",
    "                   borderMode=cv2.BORDER_TRANSPARENT,\n",
    "                   flags=cv2.WARP_INVERSE_MAP)\n",
    "    return output_im\n",
    "\n",
    "def correct_colours(im1, im2, landmarks1):\n",
    "    blur_amount = COLOUR_CORRECT_BLUR_FRAC * numpy.linalg.norm(\n",
    "                              numpy.mean(landmarks1[LEFT_EYE_POINTS], axis=0) -\n",
    "                              numpy.mean(landmarks1[RIGHT_EYE_POINTS], axis=0))\n",
    "    blur_amount = int(blur_amount)\n",
    "    if blur_amount % 2 == 0:\n",
    "        blur_amount += 1\n",
    "    im1_blur = cv2.GaussianBlur(im1, (blur_amount, blur_amount), 0)\n",
    "    im2_blur = cv2.GaussianBlur(im2, (blur_amount, blur_amount), 0)\n",
    "\n",
    "    # Avoid divide-by-zero errors.\n",
    "    im2_blur += (128 * (im2_blur <= 1.0)).astype(im2_blur.dtype)\n",
    "\n",
    "    return (im2.astype(numpy.float64) * im1_blur.astype(numpy.float64) /\n",
    "                                                im2_blur.astype(numpy.float64))\n",
    "\n",
    "def swappy(image1, image2):\n",
    "       \n",
    "    im1, landmarks1 = read_im_and_landmarks(image1)\n",
    "    im2, landmarks2 = read_im_and_landmarks(image2)\n",
    "\n",
    "    M = transformation_from_points(landmarks1[ALIGN_POINTS],\n",
    "                                   landmarks2[ALIGN_POINTS])\n",
    "    \n",
    "    mask = get_face_mask(im2, landmarks2)\n",
    "    warped_mask = warp_im(mask, M, im1.shape)\n",
    "    combined_mask = numpy.max([get_face_mask(im1, landmarks1), warped_mask],\n",
    "                              axis=0)\n",
    "\n",
    "    warped_im2 = warp_im(im2, M, im1.shape)\n",
    "    warped_corrected_im2 = correct_colours(im1, warped_im2, landmarks1)\n",
    "\n",
    "    output_im = im1 * (1.0 - combined_mask) + warped_corrected_im2 * combined_mask\n",
    "    \n",
    "    cv2.imwrite('imagens/aux_output.jpg', output_im)\n",
    "    image = cv2.imread('imagens/aux_output.jpg')\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizando a câmera para obter uma imagem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "try:\n",
    "  filename = take_photo(\"imagens/foto.jpg\")\n",
    "  print('Saved to {}'.format(filename))\n",
    "  \n",
    "  # Show the image which was just taken.\n",
    "  display(Image(filename))\n",
    "except Exception as err:\n",
    "  # Errors will be thrown if the user does not have a webcam or if they do not\n",
    "  # grant the page permission to access it.\n",
    "  print(str(err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ou utilizar imagens já salvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#image1 = cv2.imread('imagens/foto.jpg')\n",
    "\n",
    "image1 = cv2.imread('imagens/faustao-swap.jpg')\n",
    "image2 = cv2.imread('imagens/silvio-santos-swap.jpg')\n",
    "\n",
    "swapped_1 = swappy(image1, image2)\n",
    "swapped_2 = swappy(image2, image1)\n",
    "\n",
    "swapped_1 = cv2.cvtColor(swapped_1, cv2.COLOR_BGR2RGB)\n",
    "swapped_2 = cv2.cvtColor(swapped_2, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "swapped_1 = cv2.resize(swapped_1, (850, 600))\n",
    "swapped_2 = cv2.resize(swapped_2, (850, 600))\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(swapped_1)\n",
    "plt.title(\"Fausto Santos\")\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(swapped_2)\n",
    "plt.title(\"Silvo Silva\")\n",
    "plt.axis('off')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
